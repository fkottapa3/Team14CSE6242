{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5905a69",
   "metadata": {},
   "source": [
    "# CSE6242 - HW3 - Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5717e-fb7f-415c-ae02-16459c544fa4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> remove any comment that says \"#export\" because that will crash the autograder in Gradescope. We use this comment to export your code in these cells for grading.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09289981",
   "metadata": {},
   "source": [
    "Pyspark Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139318cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "### DO NOT MODIFY THIS CELL ###\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import hour, when, col, date_format, to_timestamp, ceil, coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9e0f8",
   "metadata": {},
   "source": [
    "Initialize PySpark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c18c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 03:06:41 WARN Utils: Your hostname, DESKTOP-P0T2SUH resolves to a loopback address: 127.0.1.1; using 172.31.16.68 instead (on interface eth0)\n",
      "25/03/11 03:06:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/11 03:06:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "sc = pyspark.SparkContext(appName=\"HW3-Q1\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ae314",
   "metadata": {},
   "source": [
    "Define function for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5bbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "def load_data():\n",
    "    df = sqlContext.read.option(\"header\",True) \\\n",
    "     .csv(\"yellow_tripdata_2019-01_short.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52409d",
   "metadata": {},
   "source": [
    "### Q1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f6e00",
   "metadata": {},
   "source": [
    "Perform data casting to clean incoming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#print(pyspark.__version__)\n",
    "#df.printSchema()\n",
    "#df.show(5)\n",
    "\n",
    "#df = load_data()\n",
    "#print(\"Before Casting columns\")\n",
    "#df.printSchema()\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with the all the original columns\n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    #columns_to_cast = ['passenger_count', 'total_amount', 'tip_amount', 'trip_distance', 'fare_amount', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "\n",
    "    #for columns in columns_to_cast:\n",
    "    from pyspark.sql.types import IntegerType, FloatType, TimestampType\n",
    "    from pyspark.sql.functions import col\n",
    "    \n",
    "    df = df\\\n",
    "    .withColumn(\"passenger_count\", col(\"passenger_count\").cast(IntegerType())) \\\n",
    "    .withColumn(\"total_amount\", col(\"total_amount\").cast(FloatType())) \\\n",
    "    .withColumn(\"tip_amount\", col(\"tip_amount\").cast(FloatType())) \\\n",
    "    .withColumn(\"trip_distance\", col(\"trip_distance\").cast(FloatType())) \\\n",
    "    .withColumn(\"fare_amount\", col(\"fare_amount\").cast(FloatType())) \\\n",
    "    .withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(TimestampType()))\\\n",
    "    .withColumn(\"tpep_dropoff_datetime\", col(\"tpep_dropoff_datetime\").cast(TimestampType()))\n",
    "   \n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df\n",
    "#dfCast = clean_data(df)\n",
    "#print(\"After Casting columns\")\n",
    "#dfCast.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f565d0",
   "metadata": {},
   "source": [
    "### Q1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4f712",
   "metadata": {},
   "source": [
    "Find rate per person for based on how many passengers travel between pickup and dropoff locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e115152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "def common_pair(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - PULocationID\n",
    "            - DOLocationID\n",
    "            - total_passenger_count\n",
    "            - per_person_rate\n",
    "            \n",
    "    per_person_rate is the total_amount per person for a given pair.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # START YOUR CODE HERE ---------\n",
    "\n",
    "    # Eliminate cases where the pickup and dropoff are the same\n",
    "    df_filtered = df.filter(col(\"PULocationID\") != col(\"DOLocationID\"))\n",
    "\n",
    "    # We will use PySpark aggregate function\n",
    "    # the aggregation is basically to get passenger_count, per_person_rate for the pair pickup-dropoff location\n",
    "    # per_person_rate = total_amount/total_passenger_count\n",
    "    # a. groupBy Pickup and Dropoff locations\n",
    "    # b. for each unique pickup and dropoff we will aggregate the data\n",
    "    # c. add all passenger counts for a given pickup/dropoff pair.\n",
    "    # d. calculate the total per person rate\n",
    "    \n",
    "    df_pairs = df_filtered.groupBy(\"PULocationID\", \"DOLocationID\") \\\n",
    "        .agg(sum(col(\"passenger_count\")).alias(\"total_passenger_count\"),\n",
    "            (sum(col(\"total_amount\")) / sum(col(\"passenger_count\"))).alias(\"per_person_rate\"))\n",
    "\n",
    "    df_result = df_pairs.orderBy(col(\"total_passenger_count\").desc(), col(\"per_person_rate\").desc()) \\\n",
    "                        .limit(10)\n",
    "\n",
    "    df = df_result\n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127574ab",
   "metadata": {},
   "source": [
    "### Q1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8fd27",
   "metadata": {},
   "source": [
    "Find trips which trip distances generate the highest tip percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376c981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# df = load_data()\n",
    "# print(\"Before Casting columns\")\n",
    "# df.printSchema()\n",
    "# dfCast = clean_data(df)\n",
    "# print(\"After Casting columns\")\n",
    "# dfCast.printSchema()\n",
    "# dfCommon = common_pair(dfCast)\n",
    "# print(\"After Aggregation\")\n",
    "# dfCommon.printSchema()\n",
    "# dfCommon.show(10)\n",
    "# distance_with_most_tip(df)\n",
    "\n",
    "from pyspark.sql.functions import col, round, avg, ceil\n",
    "\n",
    "\n",
    "def distance_with_most_tip(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - trip_distance\n",
    "            - tip_percent\n",
    "            \n",
    "    trip_percent is the percent of tip out of fare_amount\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    # Only include trip_distance greater than 0\n",
    "    df_filtered = df.filter((col(\"fare_amount\") > 2.00) & (col(\"trip_distance\") > 0))\n",
    "\n",
    "    # calculate tip percentatge\n",
    "    # round it to the nearest mile\n",
    "    df_tips = df_filtered.withColumn(\"tip_percent\", ceil((col(\"tip_amount\") * 100) / col(\"fare_amount\")))\n",
    "                                     \n",
    "    #                         .withColumn(\"trip_distance\", round(col(\"trip_distance\")))\n",
    "\n",
    "    # round tips\n",
    "    df_tips = df_tips.withColumn(\"trip_distance\", round(col(\"trip_distance\")))\n",
    "    \n",
    "    # groupBy trip_distance\n",
    "    # use aggregation to calculate average tip_percent\n",
    "    df_grouped = df_tips.groupBy(\"trip_distance\") \\\n",
    "                        .agg(avg(\"tip_percent\").alias(\"tip_percent\"))\n",
    "\n",
    "    # sort Descending tip_percent\n",
    "    # Select top 15\n",
    "\n",
    "    df = df_grouped.orderBy(col(\"tip_percent\").desc()).limit(15)\n",
    "    \n",
    "    # END YOUR CODE HERE -----------\n",
    "        \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5dd3a87-49c3-4cd8-b369-15ed75f7ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_data()\n",
    "# # print(\"Before Casting columns\")\n",
    "# # df.printSchema()\n",
    "# dfCast = clean_data(df)\n",
    "# # print(\"After Casting columns\")\n",
    "# # dfCast.printSchema()\n",
    "# dfCommon = common_pair(dfCast)\n",
    "# # print(\"After Aggregation\")\n",
    "# # dfCommon.printSchema()\n",
    "# dfCommon.show(10)\n",
    "# # distance_with_most_tip(dfCommon)\n",
    "# df_tip = distance_with_most_tip(df)\n",
    "# df_tip.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0172fe6",
   "metadata": {},
   "source": [
    "### Q1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613c906",
   "metadata": {},
   "source": [
    "Determine the average speed at different times of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abff9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from pyspark.sql.functions import col, hour, unix_timestamp, avg, when, date_format\n",
    "\n",
    "def time_with_most_traffic(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - time_of_day\n",
    "            - am_avg_speed\n",
    "            - pm_avg_speed\n",
    "            \n",
    "    am_avg_speed and pm_avg_speed are the average trip distance / average trip time calculated for each hour\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    # Only filter trips that have trip_distance gt 0\n",
    "    # df_filtered = df.filter(col(\"trip_distance\") > 0)\n",
    "    df_filtered = df.filter((col(\"trip_distance\") > 0) & \n",
    "                            col(\"tpep_pickup_datetime\").isNotNull() & \n",
    "                            col(\"tpep_dropoff_datetime\").isNotNull())\n",
    "    \n",
    "    # extract  hour from pickup timestamp\n",
    "    df_time = df_filtered.withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "    # calculate trip_duration in hours\n",
    "    df_time = df_time.withColumn(\"trip_duration\", \n",
    "        (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 3600)\n",
    "\n",
    "    # Formats \"1 AM\", \"12 PM\", ...\n",
    "    df_time = df_time \\\n",
    "            .withColumn(\"time_of_day\", date_format(col(\"tpep_pickup_datetime\"), \"h a\")) \\\n",
    "            .withColumn(\"am_pm\", when(col(\"hour\") < 12, \"AM\").otherwise(\"PM\"))\n",
    "\n",
    "\n",
    "\n",
    "    # df_time = df_time.withColumn(\"time_of_day\", \n",
    "    #     when(col(\"hour\") == 0, 12)\n",
    "    #    .when(col(\"hour\") > 12, col(\"hour\") - 12)\n",
    "    #    .otherwise(col(\"hour\")))\n",
    "\n",
    "\n",
    "    # Separate AM/PM\n",
    "    # df_time = df_time.withColumn(\"am_pm\", when(col(\"hour\") < 12, \"AM\").otherwise(\"PM\"))\n",
    "    \n",
    "    # convert 24 hours to 12 hr format. \n",
    "    df_time = df_time.withColumn(\"time_of_day\", when(col(\"hour\") == 0, 12)  # convert midnight to 12 AM\n",
    "                                              .when(col(\"hour\") > 12, col(\"hour\") - 12)  # convert 13 - 23 to 1-11 PM\n",
    "                                              .otherwise(col(\"hour\")))  # Keep 1-11 as-is\n",
    "   \n",
    "    # calculate average duration of each trip and average trip distance groupBy hour\n",
    "    df_avg = df_time.groupBy(\"time_of_day\", \"am_pm\") \\\n",
    "                    .agg(\n",
    "                        avg(\"trip_duration\").alias(\"avg_duration\"),\n",
    "                        avg(\"trip_distance\").alias(\"avg_distance\")\n",
    "                    )    \n",
    "    \n",
    "    \n",
    "    # calculate  average speed / hr\n",
    "    # df_avg = df_avg.withColumn(\"avg_speed\", col(\"avg_distance\") / col(\"avg_duration\"))\n",
    "    df_avg = df_avg.withColumn(\"avg_speed\", \n",
    "                               when(col(\"avg_duration\") > 0, col(\"avg_distance\") / col(\"avg_duration\"))\n",
    "                               .otherwise(None))\n",
    "    \n",
    "    \n",
    "    # Pivot to create AM & PM speed columns.\n",
    "    df_result = df_avg.groupBy(\"time_of_day\") \\\n",
    "                        .pivot(\"am_pm\", [\"AM\", \"PM\"]) \\\n",
    "                        .agg(avg(\"avg_speed\").alias(\"avg_speed\")) \\\n",
    "                        .orderBy(\"time_of_day\")\n",
    "\n",
    "    if \"AM\" in df_result.columns:\n",
    "        df_result = df_result.withColumnRenamed(\"AM\", \"am_avg_speed\")\n",
    "    if \"PM\" in df_result.columns:\n",
    "        df_result = df_result.withColumnRenamed(\"PM\", \"pm_avg_speed\")\n",
    "\n",
    "    \n",
    "   \n",
    "    # df = df_result.limit(10)\n",
    "    df = df_result.orderBy(col(\"time_of_day\").asc()).limit(10)\n",
    "\n",
    "    # \n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b4e83-0f93-4637-bc3b-34f9fbb9f249",
   "metadata": {},
   "source": [
    "## The below cells are for you to investigate your solutions and will not be graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b238c9-7bc7-458a-a3d8-8ce2d686418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bbab81e-2317-4b4e-b25a-88f3110a94f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------------------+------------------+\n",
      "|PULocationID|DOLocationID|total_passenger_count|   per_person_rate|\n",
      "+------------+------------+---------------------+------------------+\n",
      "|         239|         238|                   62|  4.26274198870505|\n",
      "|         237|         236|                   60| 4.482500068346659|\n",
      "|         263|         141|                   52|3.4190384974846473|\n",
      "|         161|         236|                   42| 5.368571440378825|\n",
      "|         148|          79|                   42| 4.711904752822149|\n",
      "|         142|         238|                   39|  5.05487182812813|\n",
      "|         141|         236|                   37| 4.355675723101641|\n",
      "|         239|         143|                   37| 4.252162224537617|\n",
      "|         239|         142|                   35| 3.817714350564139|\n",
      "|          79|         170|                   34| 6.394705884596881|\n",
      "+------------+------------+---------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_pair(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf7dd12b-4b60-407b-9c52-5b7cb2082cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|trip_distance|       tip_percent|\n",
      "+-------------+------------------+\n",
      "|          1.0|17.202070207020704|\n",
      "|          0.0|  15.6578073089701|\n",
      "|          2.0| 15.62034574468085|\n",
      "|         17.0|15.538461538461538|\n",
      "|          5.0|15.139318885448917|\n",
      "|          3.0|14.779376498800959|\n",
      "|         21.0|14.666666666666666|\n",
      "|         19.0|14.357142857142858|\n",
      "|          4.0|13.844181459566075|\n",
      "|          9.0|13.838235294117647|\n",
      "|          6.0|13.654867256637168|\n",
      "|          8.0|12.172413793103448|\n",
      "|         23.0|              12.0|\n",
      "|         18.0|             11.75|\n",
      "|         10.0|              11.7|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "distance_with_most_tip(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02723df-2490-4234-9292-eea7cebb08ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------------+\n",
      "|time_of_day|      am_avg_speed|       pm_avg_speed|\n",
      "+-----------+------------------+-------------------+\n",
      "|          1|10.845483413697353|  8.972972792548102|\n",
      "|          5|              NULL| 0.5137660239764732|\n",
      "|          6|              NULL|  9.989847870647605|\n",
      "|          7|              NULL|0.18415305490417713|\n",
      "|          8|              NULL| 0.5183127622697896|\n",
      "|         10|              NULL| 0.6147483972627696|\n",
      "|         11|              NULL|  4.651171294815548|\n",
      "|         12| 9.382686366547683|               NULL|\n",
      "+-----------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_with_most_traffic(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae02cb-daff-4c01-a88a-b1d932c36d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
